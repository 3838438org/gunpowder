import logging
import os

from gunpowder.ext import tensorflow as tf
from gunpowder.nodes.generic_train import GenericTrain
from gunpowder.volume import VolumeType, Volume

logger = logging.getLogger(__name__)

class Train(GenericTrain):
    '''Tensorflow implementation of :class:`gunpowder.nodes.Train`.

    Args:

        meta_graph_filename: Filename of a tensorflow meta-graph storing the
            tensorflow graph containing an optimizer. A meta-graph file can be
            created by running::

                # create tensorflow graph
                ...

                # store it
                tf.train.export_meta_graph(filename=meta_graph_filename)

        optimizer: The name of the tensorflow operator performing a training
            iteration.

        loss: The name of the tensorflow tensor containing the loss.

        inputs (dict): Dictionary from :class:``VolumeType`` or batch attribute
            name as string to the names of input tensors in the network.

        outputs (dict): Dictionary from :class:``VolumeType`` to the names of
            output tensors of the network. New volumes will be generated by
            this node for each entry (if requested downstream).

        gradients (dict): Dictionary from :class:``VolumeType`` to the names of
            output tensors of the network. New volumes containing the gradient
            of an output with respect to the loss will be generated by this
            node for each entry (if requested downstream).

        volume_specs (dict, optional): An optional dictionary of
            :class:`VolumeType` to :class:`VolumeSpec` to set the volume specs
            generated volumes (``outputs`` and ``gradients``). This is useful
            to set the ``voxel_size``, for example, if they differ from the
            voxel size of the input volumes. Only fields that are not ``None``
            in the given :class:`VolumeSpec` will be used.
    '''

    def __init__(
            self,
            meta_graph_filename,
            optimizer,
            loss,
            inputs,
            outputs,
            gradients,
            volume_specs=None):

        super(Train, self).__init__(
            inputs,
            outputs,
            gradients,
            volume_specs,
            spawn_subprocess=False)
        self.meta_graph_filename = meta_graph_filename
        self.optimizer = optimizer
        self.loss = loss
        self.session = None
        self.tf_gradient = {}
        self.graph = None

    def start(self):

        logger.info("Initializing tf session...")

        self.graph = tf.Graph()
        self.session = tf.Session(graph=self.graph)

        with self.graph.as_default():

            logger.info("Reading meta-graph...")

            saver = tf.train.import_meta_graph(
                self.meta_graph_filename + '.meta',
                clear_devices=True)

            checkpoint_dir = os.path.dirname(self.meta_graph_filename)
            checkpoint = tf.train.latest_checkpoint(checkpoint_dir)

            if checkpoint is not None:
                logger.info("Restoring weights from %s", checkpoint)
                saver.restore(self.session, checkpoint)
            else:
                self.session.run(tf.global_variables_initializer())

        # replace names of operations/tensors with actual operations/tensors
        self.optimizer = self.graph.get_operation_by_name(self.optimizer)
        self.loss = self.graph.get_tensor_by_name(self.loss)

        # add symbolic gradients
        for volume_type, tensor_name in self.gradients.items():
            tensor = self.graph.get_tensor_by_name(tensor_name)
            self.tf_gradient[volume_type] = tf.gradients(
                self.loss,
                [tensor])[0]

    def train_step(self, batch, request):

        to_compute = {'optimizer': self.optimizer, 'loss': self.loss}

        volume_outputs = {}
        for volume_type in self.outputs:
            if volume_type in request:
                volume_outputs[volume_type] = self.outputs[volume_type]

        for volume_type in self.gradients:
            if volume_type in request:
                volume_outputs[volume_type] = self.tf_gradient[volume_type]

        to_compute.update(volume_outputs)

        feed_dict = {}
        for network_input, input_name in self.inputs.items():
            if isinstance(network_input, VolumeType):
                feed_dict[input_name] = batch.volumes[network_input].data
            elif isinstance(network_input, str):
                feed_dict[input_name] = getattr(batch, network_input)
            else:
                raise Exception(
                    "Unknown network input type {}, can't be given to "
                    "network".format(network_input))

        outputs = self.session.run(to_compute, feed_dict=feed_dict)

        for volume_type in volume_outputs:
            spec = self.spec[volume_type].copy()
            spec.roi = request[volume_type].roi
            batch.volumes[volume_type] = Volume(
                outputs[volume_type],
                spec)

        batch.loss = outputs['loss']
        # TODO: get iteration
        batch.iteration = 0

    def stop(self):

        if self.session is not None:

            self.optimizer = self.optimizer.name
            self.loss = self.loss.name

            self.session.close()
            self.graph = None
            self.session = None
