import logging
import multiprocessing
import numpy as np
import time

from gunpowder.ext import tensorflow as tf
from gunpowder.nodes.batch_filter import BatchFilter
from gunpowder.producer_pool import ProducerPool, WorkersDied
from gunpowder.roi import Roi
from gunpowder.volume import VolumeTypes, Volume, VolumeType

logger = logging.getLogger(__name__)

class TrainProcessDied(Exception):
    pass

class Train(BatchFilter):
    '''Performs one training iteration for each batch that passes through. 
    Adds the predicted affinities to the batch.

    Args:

        optimizer: The toplevel tensorflow node performing a training iteration.

        loss: The tensorflow operator computing the loss.

        inputs (dict): Dictionary from :class:``VolumeType`` or batch attribute name as string
            to the names of input layers in the network.

        outputs (dict): Dictionary from :class:``VolumeType`` to the names of 
            output layers in the network. New volumes will be generated by this 
            node for each entry (if requested downstream).

        gradients (dict): Dictionary from :class:``VolumeType`` to the names of 
            output layers in the network. New volumes containing the gradient of 
            an output with respect to the loss will be generated by this node 
            for each entry (if requested downstream).

        use_gpu (int): Which GPU to use. Set to ``None`` for CPU mode.
    '''

    def __init__(self, optimizer, loss, inputs, outputs, gradients, use_gpu=None):

        # start training as a producer pool, so that we can gracefully exit if
        # anything goes wrong
        self.worker = ProducerPool([lambda gpu=use_gpu: self.__train(gpu)], queue_size=1)
        self.batch_in = multiprocessing.Queue(maxsize=1)

        self.optimizer = optimizer
        self.loss      = loss
        self.inputs    = inputs
        self.outputs   = outputs
        self.gradients = gradients

        self.provides = self.outputs.keys() + self.gradients.keys()

        # tf session
        self.session = None

    def setup(self):
        self.worker.start()

    def teardown(self):
        self.worker.stop()

    def prepare(self, request):

        # remove request parts that we provide
        for volume_type in self.provides:
            if volume_type in request.volumes:
                del request.volumes[volume_type]

    def process(self, batch, request):

        self.batch_in.put((batch,request))

        try:
            out = self.worker.get()
        except WorkersDied:
            raise TrainProcessDied()

        for volume_type in self.provides:
            if volume_type in request.volumes:
                batch.volumes[volume_type] = out.volumes[volume_type]
                batch.volumes[volume_type].roi = request.volumes[volume_type]

        batch.loss = out.loss
        batch.iteration = out.iteration

    def __train(self, use_gpu):

        start = time.time()

        if self.session is None:

            logger.info("Initializing tf session...")

            # TODO: use gpu
            # if use_gpu is not None:

            self.session = tf.Session()
            self.session.run(tf.initialize_all_variables())

        batch, request = self.batch_in.get()

        to_compute = { 'optimizer': self.optimizer, 'loss': self.loss }
        to_compute.update(self.outputs)

        feed_dict = {}
        for network_input, input_name in self.inputs.items():
            if isinstance(network_input, VolumeType):
                feed_dict[input_name] = batch.volumes[network_input].data
            elif isinstance(network_input, str):
                data[input_name] = getattr(batch, network_input)
            else:
                raise Exception("Unknown network input type {}, can't be given to network".format(network_input))

        outputs = self.session.run(to_compute, feed_dict=feed_dict)

        for volume_type, output_name in self.outputs.items():
            batch.volumes[volume_type] = Volume(data=outputs[volume_type])

        # TODO: how to get gradients from tf?
        # if len(self.gradients) > 0:

            # diffs = self.net_io.get_output_diffs()

            # for volume_type, output_name in self.gradients.items():
                # batch.volumes[volume_type] = Volume(
                        # data=diffs[output_name][0], # strip #batch dimension
                        # roi=Roi((0,0,0),diffs[output_name][0].shape[-3:])) # dummy roi, will be corrected in process()

        batch.loss = outputs['loss']
        # TODO: get iteration
        batch.iteration = 0

        time_of_iteration = time.time() - start
        logger.info("Train process: iteration=%d loss=%f time=%f"%(batch.iteration,batch.loss,time_of_iteration))

        return batch
