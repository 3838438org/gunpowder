import logging
import os
import numpy as np

from gunpowder.ext import tensorflow as tf
from gunpowder.nodes.generic_train import GenericTrain
from gunpowder.volume import VolumeType, Volume

logger = logging.getLogger(__name__)

class Train(GenericTrain):
    '''Tensorflow implementation of :class:`gunpowder.nodes.Train`.

    Args:

        meta_graph_filename: Filename of a tensorflow meta-graph storing the
            tensorflow graph containing an optimizer. A meta-graph file can be
            created by running::

                # create tensorflow graph
                ...

                # store it
                tf.train.export_meta_graph(filename=meta_graph_filename)

        optimizer: The name of the tensorflow operator performing a training
            iteration.

        loss: The name of the tensorflow tensor containing the loss.

        inputs (dict): Dictionary from :class:``VolumeType`` or batch attribute
            name as string to the names of input tensors in the network.

        outputs (dict): Dictionary from the names of output tensors in the
            network to :class:``VolumeType``. New volumes will be generated by
            this node for each entry (if requested downstream).

        gradients (dict): Dictionary from the names of output tensors in the
            network to :class:``VolumeType``. New volumes containing the
            gradient of an output with respect to the loss will be generated by
            this node for each entry (if requested downstream).

        volume_specs (dict, optional): An optional dictionary of
            :class:`VolumeType` to :class:`VolumeSpec` to set the volume specs
            generated volumes (``outputs`` and ``gradients``). This is useful
            to set the ``voxel_size``, for example, if they differ from the
            voxel size of the input volumes. Only fields that are not ``None``
            in the given :class:`VolumeSpec` will be used.
    '''

    def __init__(
            self,
            meta_graph_filename,
            optimizer,
            loss,
            inputs,
            outputs,
            gradients,
            volume_specs=None):

        super(Train, self).__init__(
            inputs,
            outputs,
            gradients,
            volume_specs,
            spawn_subprocess=False)
        self.meta_graph_filename = meta_graph_filename
        self.optimizer = optimizer
        self.loss = loss
        self.session = None
        self.tf_gradient = {}
        self.graph = None

    def start(self):

        logger.info("Initializing tf session...")

        self.graph = tf.Graph()
        self.session = tf.Session(graph=self.graph)

        with self.graph.as_default():

            logger.info("Reading meta-graph...")

            saver = tf.train.import_meta_graph(
                self.meta_graph_filename + '.meta',
                clear_devices=True)

            checkpoint_dir = os.path.dirname(self.meta_graph_filename)
            checkpoint = tf.train.latest_checkpoint(checkpoint_dir)

            if checkpoint is not None:
                logger.info("Restoring weights from %s", checkpoint)
                saver.restore(self.session, checkpoint)
            else:
                self.session.run(tf.global_variables_initializer())

        # replace names of operations/tensors with actual operations/tensors
        self.optimizer = self.graph.get_operation_by_name(self.optimizer)
        self.loss = self.graph.get_tensor_by_name(self.loss)

        # add symbolic gradients
        for tensor_name, volume_type in self.gradients.items():
            tensor = self.graph.get_tensor_by_name(tensor_name)
            self.tf_gradient[tensor_name] = tf.gradients(
                self.loss,
                [tensor])[0]

    def train_step(self, batch, request):

        to_compute = {'optimizer': self.optimizer, 'loss': self.loss}

        volume_outputs = {}
        for output_name, volume_type in self.outputs.items():
            if volume_type in request:
                volume_outputs[volume_type] = output_name

        for output_name, volume_type in self.gradients.items():
            if volume_type in request:
                volume_outputs[volume_type] = self.tf_gradient[output_name]

        to_compute.update(volume_outputs)

        feed_dict = {}
        for input_name, network_input in self.inputs.items():
            if isinstance(network_input, VolumeType):
                if network_input in batch.volumes:
                    feed_dict[input_name] = batch.volumes[network_input].data
                else:
                    logger.warn("batch does not contain %s, input %s will not "
                                "be set", network_input, input_name)
            elif isinstance(network_input, np.ndarray):
                feed_dict[input_name] = network_input
            elif isinstance(network_input, str):
                feed_dict[input_name] = getattr(batch, network_input)
            else:
                raise Exception(
                    "Unknown network input type {}, can't be given to "
                    "network".format(network_input))

        outputs = self.session.run(to_compute, feed_dict=feed_dict)

        for volume_type in volume_outputs:
            spec = self.spec[volume_type].copy()
            spec.roi = request[volume_type].roi
            batch.volumes[volume_type] = Volume(
                outputs[volume_type],
                spec)

        batch.loss = outputs['loss']
        # TODO: get iteration
        batch.iteration = 0

    def stop(self):

        if self.session is not None:

            self.optimizer = self.optimizer.name
            self.loss = self.loss.name

            self.session.close()
            self.graph = None
            self.session = None
